{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-18T11:42:48.244423600Z",
     "start_time": "2023-07-18T11:42:45.962534200Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get the data, the source is sited.\n",
    "# @InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "#   author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "#   title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "#   booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "#   month     = {June},\n",
    "#   year      = {2011},\n",
    "#   address   = {Portland, Oregon, USA},\n",
    "#   publisher = {Association for Computational Linguistics},\n",
    "#   pages     = {142--150},\n",
    "#   url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "# }\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_data(directory):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_type in ['neg', 'pos']:\n",
    "        dir_name = os.path.join(directory, label_type)\n",
    "        for fname in glob.glob(os.path.join(dir_name, '*.txt')):\n",
    "            with open(fname, 'r', encoding='utf-8') as f:\n",
    "                texts.append(f.read())\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "    return texts, labels\n",
    "\n",
    "train_texts, train_labels = load_data('../aclImdb_data/train')\n",
    "test_texts, test_labels = load_data('../aclImdb_data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\76219\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\76219\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\76219\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\76219\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download the NLTK data package\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Initialising word reducers and deactivators\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    preprocessed_texts = []\n",
    "    for text in texts:\n",
    "        # Text cleaning: removes non-alphabetic characters\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "        # Tokenization\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # Word Restoration and Deactivation Removal\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "\n",
    "        preprocessed_texts.append(' '.join(words))\n",
    "    return preprocessed_texts\n",
    "# Pre-processed text\n",
    "train_texts = preprocess_text(train_texts)\n",
    "test_texts = preprocess_text(test_texts)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T11:43:19.141537Z",
     "start_time": "2023-07-18T11:42:48.245423600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1310)\t2\n",
      "  (0, 4702)\t1\n",
      "  (0, 6716)\t1\n",
      "  (0, 8552)\t1\n",
      "  (0, 10879)\t1\n",
      "  (0, 11722)\t1\n",
      "  (0, 12648)\t1\n",
      "  (0, 14389)\t1\n",
      "  (0, 14794)\t1\n",
      "  (0, 16999)\t1\n",
      "  (0, 19400)\t1\n",
      "  (0, 20809)\t1\n",
      "  (0, 21215)\t1\n",
      "  (0, 21230)\t1\n",
      "  (0, 21371)\t1\n",
      "  (0, 22512)\t1\n",
      "  (0, 23859)\t1\n",
      "  (0, 23896)\t1\n",
      "  (0, 24198)\t1\n",
      "  (0, 24664)\t2\n",
      "  (0, 25265)\t1\n",
      "  (0, 26194)\t1\n",
      "  (0, 26448)\t1\n",
      "  (0, 26641)\t1\n",
      "  (0, 31512)\t1\n",
      "  :\t:\n",
      "  (0, 49245)\t1\n",
      "  (0, 53437)\t1\n",
      "  (0, 54048)\t1\n",
      "  (0, 54719)\t1\n",
      "  (0, 54728)\t1\n",
      "  (0, 55301)\t1\n",
      "  (0, 56373)\t1\n",
      "  (0, 58616)\t1\n",
      "  (0, 58677)\t1\n",
      "  (0, 58733)\t1\n",
      "  (0, 59150)\t1\n",
      "  (0, 61362)\t1\n",
      "  (0, 61687)\t1\n",
      "  (0, 61857)\t1\n",
      "  (0, 62032)\t1\n",
      "  (0, 62051)\t1\n",
      "  (0, 62401)\t1\n",
      "  (0, 63949)\t2\n",
      "  (0, 64913)\t1\n",
      "  (0, 65194)\t1\n",
      "  (0, 66636)\t1\n",
      "  (0, 66683)\t1\n",
      "  (0, 68023)\t1\n",
      "  (0, 68784)\t1\n",
      "  (0, 69812)\t1\n",
      "  (0, 3461)\t1\n",
      "  (0, 3598)\t1\n",
      "  (0, 3989)\t1\n",
      "  (0, 4207)\t1\n",
      "  (0, 4215)\t1\n",
      "  (0, 6544)\t1\n",
      "  (0, 6583)\t1\n",
      "  (0, 9844)\t3\n",
      "  (0, 10745)\t3\n",
      "  (0, 12027)\t2\n",
      "  (0, 12110)\t1\n",
      "  (0, 12222)\t1\n",
      "  (0, 12528)\t1\n",
      "  (0, 13917)\t4\n",
      "  (0, 13960)\t1\n",
      "  (0, 18444)\t1\n",
      "  (0, 18628)\t1\n",
      "  (0, 19168)\t1\n",
      "  (0, 19856)\t1\n",
      "  (0, 22034)\t1\n",
      "  (0, 22793)\t1\n",
      "  (0, 23629)\t1\n",
      "  (0, 23677)\t1\n",
      "  (0, 25357)\t2\n",
      "  (0, 27309)\t1\n",
      "  :\t:\n",
      "  (0, 43366)\t1\n",
      "  (0, 44065)\t1\n",
      "  (0, 44931)\t1\n",
      "  (0, 46733)\t1\n",
      "  (0, 47758)\t1\n",
      "  (0, 47819)\t1\n",
      "  (0, 49549)\t1\n",
      "  (0, 49555)\t1\n",
      "  (0, 50731)\t1\n",
      "  (0, 53755)\t1\n",
      "  (0, 54203)\t1\n",
      "  (0, 55047)\t1\n",
      "  (0, 55308)\t1\n",
      "  (0, 58204)\t1\n",
      "  (0, 60636)\t1\n",
      "  (0, 60823)\t1\n",
      "  (0, 60984)\t2\n",
      "  (0, 61150)\t1\n",
      "  (0, 61490)\t1\n",
      "  (0, 61762)\t1\n",
      "  (0, 63052)\t1\n",
      "  (0, 66547)\t1\n",
      "  (0, 66575)\t1\n",
      "  (0, 66760)\t1\n",
      "  (0, 67426)\t1\n"
     ]
    }
   ],
   "source": [
    "#Form the vocabulary list & Vectorize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def vectorize(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    vectorizer.fit(texts)\n",
    "    # vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "    sequences = vectorizer.transform(texts)\n",
    "\n",
    "    return sequences\n",
    "\n",
    "train_sequences = vectorize(train_texts)\n",
    "test_sequences = vectorize(test_texts)\n",
    "print(train_sequences[0])\n",
    "print(test_sequences[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-18T11:43:23.835660Z",
     "start_time": "2023-07-18T11:43:19.203585400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
