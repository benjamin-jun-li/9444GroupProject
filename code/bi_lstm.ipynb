{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-24T19:57:41.005355200Z",
     "start_time": "2023-07-24T19:57:04.856495200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.5077714228630066\n",
      "Validation loss decreased (0.460342 --> 0.460342).  Saving model ...\n",
      "Epoch 2, Loss: 0.45646918864250186\n",
      "Validation loss decreased (0.449592 --> 0.449592).  Saving model ...\n",
      "Epoch 3, Loss: 0.4494832589387894\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 4, Loss: 0.4444856627225876\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 5, Loss: 0.4404243407249451\n",
      "Validation loss decreased (0.436847 --> 0.436847).  Saving model ...\n",
      "Epoch 6, Loss: 0.4370722928762436\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 7, Loss: 0.4341897609949112\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 8, Loss: 0.43155703101158144\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 9, Loss: 0.4290436578273773\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 10, Loss: 0.4265789274692535\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Test Loss: 0.4347554003953934, Accuracy: 0.77732\n"
     ]
    }
   ],
   "source": [
    "from dataProcessing import *\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits as bce_loss\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.delta = delta\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        torch.save(model.state_dict(), 'checkpoint.pt')\n",
    "        print(f'Validation loss decreased ({-self.best_score:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, bidirectional, dropout):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers,\n",
    "                            bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim*2 if bidirectional else hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1) # Add sequence length dimension\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        else:\n",
    "            hidden = hidden[-1,:,:]\n",
    "        output = self.fc(hidden).squeeze()\n",
    "        return output\n",
    "\n",
    "batch_size = 32\n",
    "train_texts = np.load('../p_data/train_texts.npy', allow_pickle=True)\n",
    "train_labels = np.load('../p_data/train_labels.npy', allow_pickle=True)\n",
    "test_texts = np.load('../p_data/test_texts.npy', allow_pickle=True)\n",
    "test_labels = np.load('../p_data/test_labels.npy', allow_pickle=True)\n",
    "\n",
    "w2v_model = w2v_train(train_texts)\n",
    "\n",
    "# Convert texts to vectors\n",
    "train_data = [text_to_vec(text, w2v_model) for text in train_texts]\n",
    "test_data = [text_to_vec(text, w2v_model) for text in test_texts]\n",
    "\n",
    "# Divide the training set and validation set\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    train_data, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_data = [torch.tensor(vec) for vec in train_data]\n",
    "val_data = [torch.tensor(vec) for vec in val_data]\n",
    "test_data = [torch.tensor(vec) for vec in test_data]\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(VectorDataset(train_data, train_labels), batch_size)\n",
    "val_loader = DataLoader(VectorDataset(val_data, val_labels), batch_size)\n",
    "test_loader = DataLoader(VectorDataset(test_data, test_labels), batch_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.01)\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "dropout = 0.5\n",
    "model = Classifier(input_dim=100, hidden_dim=hidden_dim, num_layers=num_layers,\n",
    "                   bidirectional=bidirectional, dropout=dropout).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs.float())\n",
    "        loss = bce_loss(outputs, targets.float())\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_count += inputs.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = total_loss / total_count\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs.float())\n",
    "            loss = bce_loss(outputs, targets.float())\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_count += inputs.size(0)\n",
    "    val_loss /= val_count\n",
    "\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        outputs = model(inputs.float())\n",
    "        loss = bce_loss(outputs, targets.float())\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_count += inputs.size(0)\n",
    "        pred = (outputs > 0.5).long()\n",
    "        correct_count += (pred == targets).sum().item()\n",
    "avg_loss = total_loss / total_count\n",
    "accuracy = correct_count / total_count\n",
    "print(f'Test Loss: {avg_loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\76219\\AppData\\Local\\Temp\\ipykernel_31448\\3156636974.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = [torch.tensor(vec) for vec in train_data]\n",
      "C:\\Users\\76219\\AppData\\Local\\Temp\\ipykernel_31448\\3156636974.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_data = [torch.tensor(vec) for vec in val_data]\n",
      "C:\\Users\\76219\\AppData\\Local\\Temp\\ipykernel_31448\\3156636974.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = [torch.tensor(vec) for vec in test_data]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6055511006333696\n",
      "Validation loss decreased (0.552904 --> 0.552904).  Saving model ...\n",
      "Epoch 2, Loss: 0.5474187473967892\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 3, Loss: 0.5299750655819716\n",
      "Validation loss decreased (0.500112 --> 0.500112).  Saving model ...\n",
      "Epoch 4, Loss: 0.5071744214827711\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 5, Loss: 0.49003544345114214\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 6, Loss: 0.44576130850439794\n",
      "Validation loss decreased (0.398613 --> 0.398613).  Saving model ...\n",
      "Epoch 7, Loss: 0.35255462219713257\n",
      "Validation loss decreased (0.292701 --> 0.292701).  Saving model ...\n",
      "Epoch 8, Loss: 0.2710202895599264\n",
      "Validation loss decreased (0.234735 --> 0.234735).  Saving model ...\n",
      "Epoch 9, Loss: 0.175788465465632\n",
      "Validation loss decreased (0.167042 --> 0.167042).  Saving model ...\n",
      "Epoch 10, Loss: 0.13381583510573072\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 11, Loss: 0.05499133537468921\n",
      "Validation loss decreased (-0.126494 --> -0.126494).  Saving model ...\n",
      "Epoch 12, Loss: -0.17423718325520382\n",
      "Validation loss decreased (-0.247355 --> -0.247355).  Saving model ...\n",
      "Epoch 13, Loss: -0.1324423671140976\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 14, Loss: -0.3681302408656508\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 15, Loss: -0.3384922800540552\n",
      "Validation loss decreased (-0.449877 --> -0.449877).  Saving model ...\n",
      "Epoch 16, Loss: -0.6190597885619096\n",
      "Validation loss decreased (-0.709903 --> -0.709903).  Saving model ...\n",
      "Epoch 17, Loss: -0.7242963849148836\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 18, Loss: -0.8999232727042592\n",
      "Validation loss decreased (-0.883866 --> -0.883866).  Saving model ...\n",
      "Epoch 19, Loss: -0.6114761596727334\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Epoch 20, Loss: 10.961164469834326\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Epoch 21, Loss: 1.677781220156173\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Epoch 22, Loss: 0.6916358851995624\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Epoch 23, Loss: 0.6682690925006286\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n",
      "Test Loss: -0.5851067545635452, Accuracy: 0.651183970856102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from dataProcessing_t import *\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "batch_size = 128\n",
    "train_texts = np.load('../p_data_3/train_texts.npy', allow_pickle=True)\n",
    "train_labels = np.load('../p_data_3/train_labels.npy', allow_pickle=True)\n",
    "test_texts = np.load('../p_data_3/test_texts.npy', allow_pickle=True)\n",
    "test_labels = np.load('../p_data_3/test_labels.npy', allow_pickle=True)\n",
    "val_texts = np.load('../p_data_3/val_texts.npy', allow_pickle=True)\n",
    "val_labels = np.load('../p_data_3/val_labels.npy', allow_pickle=True)\n",
    "\n",
    "# Create a Word2Vec model\n",
    "w2v_model = Word2Vec(train_texts, min_count=1, vector_size=100)\n",
    "\n",
    "def get_word_vector(word, w2v_model, default_vector):\n",
    "    try:\n",
    "        return w2v_model.wv[word] if word in w2v_model.wv else default_vector\n",
    "    except KeyError:\n",
    "        return default_vector\n",
    "\n",
    "# Padding function\n",
    "def pad_texts(texts):\n",
    "    return pad_sequence([torch.tensor([get_word_vector(word, w2v_model, default_vector) for word in text]) for text in texts], batch_first=True)\n",
    "\n",
    "\n",
    "# Calculate the default vector\n",
    "default_vector = np.mean(w2v_model.wv.vectors, axis=0)\n",
    "\n",
    "# Convert the texts to vectors with padding\n",
    "train_data = pad_texts(train_texts)\n",
    "val_data = pad_texts(val_texts)\n",
    "test_data = pad_texts(test_texts)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(train_labels)\n",
    "val_labels = label_encoder.transform(val_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Convert lists to tensors\n",
    "train_data = [torch.tensor(vec) for vec in train_data]\n",
    "val_data = [torch.tensor(vec) for vec in val_data]\n",
    "test_data = [torch.tensor(vec) for vec in test_data]\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "train_loader = DataLoader(list(zip(train_data, train_labels)), batch_size)\n",
    "val_loader = DataLoader(list(zip(val_data, val_labels)), batch_size)\n",
    "test_loader = DataLoader(list(zip(test_data, test_labels)), batch_size)\n",
    "\n",
    "\n",
    "# Define the LSTM Classifier\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        self.fc = nn.Linear(2*hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Split the final state into the forward and backward parts\n",
    "        forward_final = lstm_out[:, -1, :self.hidden_dim]\n",
    "        backward_final = lstm_out[:, 0, self.hidden_dim:]\n",
    "        # Concatenate the final states and pass through the linear layer\n",
    "        out = self.fc(torch.cat((forward_final, backward_final), dim=1)).squeeze()\n",
    "        return out\n",
    "\n",
    "# Define your LSTM model\n",
    "embedding_dim = 100  # This should match the dimension of your word2vec vectors\n",
    "hidden_dim = 100\n",
    "model = BiLSTMClassifier(embedding_dim, hidden_dim).to(device)\n",
    "early_stopping = EarlyStopping(patience=5, delta=0.01)\n",
    "\n",
    "# Define loss function\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "model.train()\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    total_count = 0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device).float()\n",
    "        model.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_function(outputs, targets)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_count += inputs.size(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_loss = total_loss / total_count\n",
    "    print(f'Epoch {epoch+1}, Loss: {avg_loss}')\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_count = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).float()\n",
    "            outputs = model(inputs.float())\n",
    "            loss = loss_function(outputs, targets)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_count += inputs.size(0)\n",
    "    val_loss /= val_count\n",
    "\n",
    "    # Update the early stopping object\n",
    "    early_stopping(val_loss, model)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "total_loss = 0\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device).float()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = loss_function(outputs, targets)\n",
    "        total_loss += loss.item() * inputs.size(0)\n",
    "        total_count += inputs.size(0)\n",
    "        pred = (torch.sigmoid(outputs) > 0.5).long()\n",
    "        correct_count += (pred == targets.long()).sum().item()\n",
    "avg_loss = total_loss / total_count\n",
    "accuracy = correct_count / total_count\n",
    "print(f'Test Loss: {avg_loss}, Accuracy: {accuracy}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-24T20:00:58.946764700Z",
     "start_time": "2023-07-24T20:00:16.019372800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
